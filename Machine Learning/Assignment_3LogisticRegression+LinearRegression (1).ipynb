{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement logistic regression algorithm from scratch (preferably in python) and test your code on a tiny\n",
        "dataset of your choice. You might want to break down the full algorithm into small functions."
      ],
      "metadata": {
        "id": "oOUx7yurHX6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "# Make a prediction with coefficients\n",
        "def predict(row, w):\n",
        "  yhat = w[-1] # bias is at dimnesion 0\n",
        "  for i in range(len(row)-1):\n",
        "    yhat += w[i] * row[i]\n",
        "  return 1.0 / (1.0 + exp(-yhat))\n",
        "\n",
        "# test step 1\n",
        "dataset = [[2.7810836,2.550537003,0], [1.465489372,2.362125076,0],\n",
        " [3.396561688,4.400293529,0], [1.38807019,1.850220317,0],\n",
        " [3.06407232,3.005305973,0], [7.627531214,2.759262235,1],\n",
        " [5.332441248,2.088626775,1], [6.922596716,1.77106367,1],\n",
        " [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1]\n",
        " ]\n",
        "\n",
        "weights = [0.6067, -0.8446,-0.2826] # these are final weights\n",
        "for row in dataset:\n",
        "  yhat = predict(row, weights)\n",
        "  print(\"Expected=%.3f, Predicted=%.3f [%d]\"% (row[-1], yhat, round(yhat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADedRprjHqyx",
        "outputId": "f0da8076-602d-4d6a-ed79-2143823d75d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=0.000, Predicted=0.321 [0]\n",
            "Expected=0.000, Predicted=0.200 [0]\n",
            "Expected=0.000, Predicted=0.126 [0]\n",
            "Expected=0.000, Predicted=0.268 [0]\n",
            "Expected=0.000, Predicted=0.276 [0]\n",
            "Expected=1.000, Predicted=0.882 [1]\n",
            "Expected=1.000, Predicted=0.766 [1]\n",
            "Expected=1.000, Predicted=0.918 [1]\n",
            "Expected=1.000, Predicted=0.994 [1]\n",
            "Expected=1.000, Predicted=0.804 [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate logistic regression coefficients\n",
        "#using stochastic gradient descent\n",
        "def weights_sgd(train, l_rate, n_epoch):\n",
        "  w = [0.0 for i in range(len(train[0]))]\n",
        "  for epoch in range(n_epoch):\n",
        "    sum_error = 0\n",
        "    for row in train:\n",
        "      yhat = predict(row, w)\n",
        "      error = row[-1] - yhat\n",
        "      sum_error += error**2\n",
        "      w[-1] = w[-1] + l_rate * \\\n",
        "      error * yhat * (1.0 - yhat)\n",
        "      for i in range(len(row)-1):\n",
        "        w[i] = w[i] + l_rate * error * \\\n",
        "        yhat * (1.0 - yhat) * row[i]\n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f'% (epoch, l_rate, sum_error))\n",
        "  return w\n",
        "\n",
        "l_rate = 0.03\n",
        "n_epoch = 50\n",
        "w = weights_sgd(dataset, l_rate, n_epoch)\n",
        "print(w)\n",
        "\n",
        "for row in dataset:\n",
        "  yhat = predict(row, w)\n",
        "  print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HwLmd3aI6eV",
        "outputId": "b700aa15-0296-490c-82eb-b096c8ae6c17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=0, lrate=0.030, error=2.371\n",
            ">epoch=1, lrate=0.030, error=2.083\n",
            ">epoch=2, lrate=0.030, error=1.948\n",
            ">epoch=3, lrate=0.030, error=1.841\n",
            ">epoch=4, lrate=0.030, error=1.740\n",
            ">epoch=5, lrate=0.030, error=1.646\n",
            ">epoch=6, lrate=0.030, error=1.559\n",
            ">epoch=7, lrate=0.030, error=1.479\n",
            ">epoch=8, lrate=0.030, error=1.405\n",
            ">epoch=9, lrate=0.030, error=1.338\n",
            ">epoch=10, lrate=0.030, error=1.276\n",
            ">epoch=11, lrate=0.030, error=1.219\n",
            ">epoch=12, lrate=0.030, error=1.166\n",
            ">epoch=13, lrate=0.030, error=1.118\n",
            ">epoch=14, lrate=0.030, error=1.073\n",
            ">epoch=15, lrate=0.030, error=1.031\n",
            ">epoch=16, lrate=0.030, error=0.993\n",
            ">epoch=17, lrate=0.030, error=0.956\n",
            ">epoch=18, lrate=0.030, error=0.923\n",
            ">epoch=19, lrate=0.030, error=0.891\n",
            ">epoch=20, lrate=0.030, error=0.862\n",
            ">epoch=21, lrate=0.030, error=0.834\n",
            ">epoch=22, lrate=0.030, error=0.808\n",
            ">epoch=23, lrate=0.030, error=0.783\n",
            ">epoch=24, lrate=0.030, error=0.760\n",
            ">epoch=25, lrate=0.030, error=0.738\n",
            ">epoch=26, lrate=0.030, error=0.718\n",
            ">epoch=27, lrate=0.030, error=0.698\n",
            ">epoch=28, lrate=0.030, error=0.679\n",
            ">epoch=29, lrate=0.030, error=0.662\n",
            ">epoch=30, lrate=0.030, error=0.645\n",
            ">epoch=31, lrate=0.030, error=0.629\n",
            ">epoch=32, lrate=0.030, error=0.613\n",
            ">epoch=33, lrate=0.030, error=0.599\n",
            ">epoch=34, lrate=0.030, error=0.585\n",
            ">epoch=35, lrate=0.030, error=0.572\n",
            ">epoch=36, lrate=0.030, error=0.559\n",
            ">epoch=37, lrate=0.030, error=0.547\n",
            ">epoch=38, lrate=0.030, error=0.535\n",
            ">epoch=39, lrate=0.030, error=0.524\n",
            ">epoch=40, lrate=0.030, error=0.513\n",
            ">epoch=41, lrate=0.030, error=0.503\n",
            ">epoch=42, lrate=0.030, error=0.493\n",
            ">epoch=43, lrate=0.030, error=0.483\n",
            ">epoch=44, lrate=0.030, error=0.474\n",
            ">epoch=45, lrate=0.030, error=0.465\n",
            ">epoch=46, lrate=0.030, error=0.456\n",
            ">epoch=47, lrate=0.030, error=0.448\n",
            ">epoch=48, lrate=0.030, error=0.440\n",
            ">epoch=49, lrate=0.030, error=0.433\n",
            "[0.6067464074091063, -0.8446034365392575, -0.28269453089059016]\n",
            "Expected=0.000, Predicted=0.321 [0]\n",
            "Expected=0.000, Predicted=0.200 [0]\n",
            "Expected=0.000, Predicted=0.126 [0]\n",
            "Expected=0.000, Predicted=0.268 [0]\n",
            "Expected=0.000, Predicted=0.277 [0]\n",
            "Expected=1.000, Predicted=0.882 [1]\n",
            "Expected=1.000, Predicted=0.767 [1]\n",
            "Expected=1.000, Predicted=0.918 [1]\n",
            "Expected=1.000, Predicted=0.994 [1]\n",
            "Expected=1.000, Predicted=0.804 [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "  minmax = list()\n",
        "  for i in range(len(dataset[0])):\n",
        "    col_values = [row[i] for row in dataset]\n",
        "    value_min = min(col_values)\n",
        "    value_max = max(col_values)\n",
        "    minmax.append([value_min, value_max])\n",
        "  return minmax\n",
        "\n",
        "def normalize_dataset(dataset, minmax):\n",
        " for row in dataset:\n",
        "  for i in range(len(row)):\n",
        "    row[i] = (row[i] - minmax[i][0]) \\\n",
        "    / (minmax[i][1] - minmax[i][0])\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "  dataset_split = list()\n",
        "  dataset_copy = list(dataset)\n",
        "  fold_size = int(len(dataset) / n_folds)\n",
        "  for i in range(n_folds):\n",
        "    fold = list()\n",
        "    while len(fold) < fold_size:\n",
        "      index = random.randrange(len(dataset_copy))\n",
        "      fold.append(dataset_copy.pop(index))\n",
        "    dataset_split.append(fold)\n",
        "  return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "  correct = 0\n",
        "  for i in range(len(actual)):\n",
        "    if actual[i] == predicted[i]:\n",
        "      correct += 1\n",
        "  return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Calculate root mean squared error\n",
        "def rmse_metric(actual, predicted):\n",
        "  sum_error = 0.0\n",
        "  for i in range(len(actual)):\n",
        "    prediction_error = predicted[i] - actual[i]\n",
        "    sum_error += (prediction_error ** 2)\n",
        "  mean_error = sum_error / float(len(actual))\n",
        "  return sqrt(mean_error)\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "  folds = cross_validation_split(dataset, n_folds)\n",
        "  scores = list()\n",
        "  for fold in folds:\n",
        "    train_set = list(folds)\n",
        "    train_set.remove(fold)\n",
        "    train_set = sum(train_set, [])\n",
        "    test_set = list()\n",
        "    for row in fold:\n",
        "      row_copy = list(row)\n",
        "      test_set.append(row_copy)\n",
        "      row_copy[-1] = None\n",
        "    predicted = algorithm(train_set, test_set, *args)\n",
        "    actual = [row[-1] for row in fold]\n",
        "    accuracy = accuracy_metric(actual, predicted)\n",
        "    scores.append(accuracy)\n",
        "  return scores\n",
        "\n",
        "def logistic_regression(train, test, l_rate, n_epoch):\n",
        " predictions = list()\n",
        " w = weights_sgd(train, l_rate, n_epoch)\n",
        " for row in test:\n",
        "  yhat = predict(row, w)\n",
        "  yhat = round(yhat)\n",
        "  predictions.append(yhat)\n",
        " return(predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "t6fUINpbKDmb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.1\n",
        "n_epoch = 50\n",
        "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%'% (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89A7_0ZnMyyx",
        "outputId": "2d8baf72-7c96-4962-fde8-a62c24ecf1d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=0, lrate=0.100, error=2.009\n",
            ">epoch=1, lrate=0.100, error=1.984\n",
            ">epoch=2, lrate=0.100, error=1.960\n",
            ">epoch=3, lrate=0.100, error=1.936\n",
            ">epoch=4, lrate=0.100, error=1.913\n",
            ">epoch=5, lrate=0.100, error=1.890\n",
            ">epoch=6, lrate=0.100, error=1.868\n",
            ">epoch=7, lrate=0.100, error=1.846\n",
            ">epoch=8, lrate=0.100, error=1.824\n",
            ">epoch=9, lrate=0.100, error=1.803\n",
            ">epoch=10, lrate=0.100, error=1.782\n",
            ">epoch=11, lrate=0.100, error=1.762\n",
            ">epoch=12, lrate=0.100, error=1.742\n",
            ">epoch=13, lrate=0.100, error=1.723\n",
            ">epoch=14, lrate=0.100, error=1.703\n",
            ">epoch=15, lrate=0.100, error=1.684\n",
            ">epoch=16, lrate=0.100, error=1.666\n",
            ">epoch=17, lrate=0.100, error=1.648\n",
            ">epoch=18, lrate=0.100, error=1.630\n",
            ">epoch=19, lrate=0.100, error=1.612\n",
            ">epoch=20, lrate=0.100, error=1.595\n",
            ">epoch=21, lrate=0.100, error=1.578\n",
            ">epoch=22, lrate=0.100, error=1.561\n",
            ">epoch=23, lrate=0.100, error=1.545\n",
            ">epoch=24, lrate=0.100, error=1.528\n",
            ">epoch=25, lrate=0.100, error=1.513\n",
            ">epoch=26, lrate=0.100, error=1.497\n",
            ">epoch=27, lrate=0.100, error=1.482\n",
            ">epoch=28, lrate=0.100, error=1.467\n",
            ">epoch=29, lrate=0.100, error=1.452\n",
            ">epoch=30, lrate=0.100, error=1.437\n",
            ">epoch=31, lrate=0.100, error=1.423\n",
            ">epoch=32, lrate=0.100, error=1.409\n",
            ">epoch=33, lrate=0.100, error=1.395\n",
            ">epoch=34, lrate=0.100, error=1.382\n",
            ">epoch=35, lrate=0.100, error=1.368\n",
            ">epoch=36, lrate=0.100, error=1.355\n",
            ">epoch=37, lrate=0.100, error=1.342\n",
            ">epoch=38, lrate=0.100, error=1.330\n",
            ">epoch=39, lrate=0.100, error=1.317\n",
            ">epoch=40, lrate=0.100, error=1.305\n",
            ">epoch=41, lrate=0.100, error=1.293\n",
            ">epoch=42, lrate=0.100, error=1.281\n",
            ">epoch=43, lrate=0.100, error=1.269\n",
            ">epoch=44, lrate=0.100, error=1.258\n",
            ">epoch=45, lrate=0.100, error=1.247\n",
            ">epoch=46, lrate=0.100, error=1.236\n",
            ">epoch=47, lrate=0.100, error=1.225\n",
            ">epoch=48, lrate=0.100, error=1.214\n",
            ">epoch=49, lrate=0.100, error=1.203\n",
            ">epoch=0, lrate=0.100, error=2.009\n",
            ">epoch=1, lrate=0.100, error=1.984\n",
            ">epoch=2, lrate=0.100, error=1.959\n",
            ">epoch=3, lrate=0.100, error=1.935\n",
            ">epoch=4, lrate=0.100, error=1.911\n",
            ">epoch=5, lrate=0.100, error=1.888\n",
            ">epoch=6, lrate=0.100, error=1.865\n",
            ">epoch=7, lrate=0.100, error=1.843\n",
            ">epoch=8, lrate=0.100, error=1.822\n",
            ">epoch=9, lrate=0.100, error=1.800\n",
            ">epoch=10, lrate=0.100, error=1.780\n",
            ">epoch=11, lrate=0.100, error=1.759\n",
            ">epoch=12, lrate=0.100, error=1.739\n",
            ">epoch=13, lrate=0.100, error=1.720\n",
            ">epoch=14, lrate=0.100, error=1.700\n",
            ">epoch=15, lrate=0.100, error=1.681\n",
            ">epoch=16, lrate=0.100, error=1.663\n",
            ">epoch=17, lrate=0.100, error=1.645\n",
            ">epoch=18, lrate=0.100, error=1.627\n",
            ">epoch=19, lrate=0.100, error=1.609\n",
            ">epoch=20, lrate=0.100, error=1.592\n",
            ">epoch=21, lrate=0.100, error=1.575\n",
            ">epoch=22, lrate=0.100, error=1.558\n",
            ">epoch=23, lrate=0.100, error=1.542\n",
            ">epoch=24, lrate=0.100, error=1.525\n",
            ">epoch=25, lrate=0.100, error=1.509\n",
            ">epoch=26, lrate=0.100, error=1.494\n",
            ">epoch=27, lrate=0.100, error=1.479\n",
            ">epoch=28, lrate=0.100, error=1.463\n",
            ">epoch=29, lrate=0.100, error=1.449\n",
            ">epoch=30, lrate=0.100, error=1.434\n",
            ">epoch=31, lrate=0.100, error=1.420\n",
            ">epoch=32, lrate=0.100, error=1.406\n",
            ">epoch=33, lrate=0.100, error=1.392\n",
            ">epoch=34, lrate=0.100, error=1.378\n",
            ">epoch=35, lrate=0.100, error=1.365\n",
            ">epoch=36, lrate=0.100, error=1.351\n",
            ">epoch=37, lrate=0.100, error=1.338\n",
            ">epoch=38, lrate=0.100, error=1.326\n",
            ">epoch=39, lrate=0.100, error=1.313\n",
            ">epoch=40, lrate=0.100, error=1.301\n",
            ">epoch=41, lrate=0.100, error=1.289\n",
            ">epoch=42, lrate=0.100, error=1.277\n",
            ">epoch=43, lrate=0.100, error=1.265\n",
            ">epoch=44, lrate=0.100, error=1.253\n",
            ">epoch=45, lrate=0.100, error=1.242\n",
            ">epoch=46, lrate=0.100, error=1.231\n",
            ">epoch=47, lrate=0.100, error=1.220\n",
            ">epoch=48, lrate=0.100, error=1.209\n",
            ">epoch=49, lrate=0.100, error=1.198\n",
            ">epoch=0, lrate=0.100, error=1.995\n",
            ">epoch=1, lrate=0.100, error=1.945\n",
            ">epoch=2, lrate=0.100, error=1.901\n",
            ">epoch=3, lrate=0.100, error=1.861\n",
            ">epoch=4, lrate=0.100, error=1.826\n",
            ">epoch=5, lrate=0.100, error=1.794\n",
            ">epoch=6, lrate=0.100, error=1.766\n",
            ">epoch=7, lrate=0.100, error=1.740\n",
            ">epoch=8, lrate=0.100, error=1.716\n",
            ">epoch=9, lrate=0.100, error=1.694\n",
            ">epoch=10, lrate=0.100, error=1.674\n",
            ">epoch=11, lrate=0.100, error=1.655\n",
            ">epoch=12, lrate=0.100, error=1.637\n",
            ">epoch=13, lrate=0.100, error=1.620\n",
            ">epoch=14, lrate=0.100, error=1.605\n",
            ">epoch=15, lrate=0.100, error=1.589\n",
            ">epoch=16, lrate=0.100, error=1.575\n",
            ">epoch=17, lrate=0.100, error=1.561\n",
            ">epoch=18, lrate=0.100, error=1.547\n",
            ">epoch=19, lrate=0.100, error=1.534\n",
            ">epoch=20, lrate=0.100, error=1.521\n",
            ">epoch=21, lrate=0.100, error=1.509\n",
            ">epoch=22, lrate=0.100, error=1.497\n",
            ">epoch=23, lrate=0.100, error=1.485\n",
            ">epoch=24, lrate=0.100, error=1.473\n",
            ">epoch=25, lrate=0.100, error=1.462\n",
            ">epoch=26, lrate=0.100, error=1.451\n",
            ">epoch=27, lrate=0.100, error=1.440\n",
            ">epoch=28, lrate=0.100, error=1.429\n",
            ">epoch=29, lrate=0.100, error=1.418\n",
            ">epoch=30, lrate=0.100, error=1.407\n",
            ">epoch=31, lrate=0.100, error=1.397\n",
            ">epoch=32, lrate=0.100, error=1.386\n",
            ">epoch=33, lrate=0.100, error=1.376\n",
            ">epoch=34, lrate=0.100, error=1.366\n",
            ">epoch=35, lrate=0.100, error=1.356\n",
            ">epoch=36, lrate=0.100, error=1.346\n",
            ">epoch=37, lrate=0.100, error=1.336\n",
            ">epoch=38, lrate=0.100, error=1.326\n",
            ">epoch=39, lrate=0.100, error=1.317\n",
            ">epoch=40, lrate=0.100, error=1.307\n",
            ">epoch=41, lrate=0.100, error=1.298\n",
            ">epoch=42, lrate=0.100, error=1.289\n",
            ">epoch=43, lrate=0.100, error=1.280\n",
            ">epoch=44, lrate=0.100, error=1.270\n",
            ">epoch=45, lrate=0.100, error=1.261\n",
            ">epoch=46, lrate=0.100, error=1.253\n",
            ">epoch=47, lrate=0.100, error=1.244\n",
            ">epoch=48, lrate=0.100, error=1.235\n",
            ">epoch=49, lrate=0.100, error=1.226\n",
            ">epoch=0, lrate=0.100, error=2.002\n",
            ">epoch=1, lrate=0.100, error=1.964\n",
            ">epoch=2, lrate=0.100, error=1.928\n",
            ">epoch=3, lrate=0.100, error=1.894\n",
            ">epoch=4, lrate=0.100, error=1.861\n",
            ">epoch=5, lrate=0.100, error=1.830\n",
            ">epoch=6, lrate=0.100, error=1.800\n",
            ">epoch=7, lrate=0.100, error=1.772\n",
            ">epoch=8, lrate=0.100, error=1.744\n",
            ">epoch=9, lrate=0.100, error=1.718\n",
            ">epoch=10, lrate=0.100, error=1.693\n",
            ">epoch=11, lrate=0.100, error=1.668\n",
            ">epoch=12, lrate=0.100, error=1.644\n",
            ">epoch=13, lrate=0.100, error=1.621\n",
            ">epoch=14, lrate=0.100, error=1.598\n",
            ">epoch=15, lrate=0.100, error=1.576\n",
            ">epoch=16, lrate=0.100, error=1.555\n",
            ">epoch=17, lrate=0.100, error=1.534\n",
            ">epoch=18, lrate=0.100, error=1.513\n",
            ">epoch=19, lrate=0.100, error=1.493\n",
            ">epoch=20, lrate=0.100, error=1.474\n",
            ">epoch=21, lrate=0.100, error=1.455\n",
            ">epoch=22, lrate=0.100, error=1.436\n",
            ">epoch=23, lrate=0.100, error=1.418\n",
            ">epoch=24, lrate=0.100, error=1.400\n",
            ">epoch=25, lrate=0.100, error=1.382\n",
            ">epoch=26, lrate=0.100, error=1.365\n",
            ">epoch=27, lrate=0.100, error=1.348\n",
            ">epoch=28, lrate=0.100, error=1.332\n",
            ">epoch=29, lrate=0.100, error=1.316\n",
            ">epoch=30, lrate=0.100, error=1.300\n",
            ">epoch=31, lrate=0.100, error=1.284\n",
            ">epoch=32, lrate=0.100, error=1.269\n",
            ">epoch=33, lrate=0.100, error=1.254\n",
            ">epoch=34, lrate=0.100, error=1.239\n",
            ">epoch=35, lrate=0.100, error=1.225\n",
            ">epoch=36, lrate=0.100, error=1.210\n",
            ">epoch=37, lrate=0.100, error=1.196\n",
            ">epoch=38, lrate=0.100, error=1.183\n",
            ">epoch=39, lrate=0.100, error=1.169\n",
            ">epoch=40, lrate=0.100, error=1.156\n",
            ">epoch=41, lrate=0.100, error=1.143\n",
            ">epoch=42, lrate=0.100, error=1.131\n",
            ">epoch=43, lrate=0.100, error=1.118\n",
            ">epoch=44, lrate=0.100, error=1.106\n",
            ">epoch=45, lrate=0.100, error=1.094\n",
            ">epoch=46, lrate=0.100, error=1.082\n",
            ">epoch=47, lrate=0.100, error=1.071\n",
            ">epoch=48, lrate=0.100, error=1.059\n",
            ">epoch=49, lrate=0.100, error=1.048\n",
            ">epoch=0, lrate=0.100, error=2.013\n",
            ">epoch=1, lrate=0.100, error=1.996\n",
            ">epoch=2, lrate=0.100, error=1.979\n",
            ">epoch=3, lrate=0.100, error=1.962\n",
            ">epoch=4, lrate=0.100, error=1.945\n",
            ">epoch=5, lrate=0.100, error=1.929\n",
            ">epoch=6, lrate=0.100, error=1.913\n",
            ">epoch=7, lrate=0.100, error=1.897\n",
            ">epoch=8, lrate=0.100, error=1.882\n",
            ">epoch=9, lrate=0.100, error=1.866\n",
            ">epoch=10, lrate=0.100, error=1.851\n",
            ">epoch=11, lrate=0.100, error=1.836\n",
            ">epoch=12, lrate=0.100, error=1.822\n",
            ">epoch=13, lrate=0.100, error=1.807\n",
            ">epoch=14, lrate=0.100, error=1.793\n",
            ">epoch=15, lrate=0.100, error=1.778\n",
            ">epoch=16, lrate=0.100, error=1.764\n",
            ">epoch=17, lrate=0.100, error=1.750\n",
            ">epoch=18, lrate=0.100, error=1.736\n",
            ">epoch=19, lrate=0.100, error=1.723\n",
            ">epoch=20, lrate=0.100, error=1.709\n",
            ">epoch=21, lrate=0.100, error=1.696\n",
            ">epoch=22, lrate=0.100, error=1.682\n",
            ">epoch=23, lrate=0.100, error=1.669\n",
            ">epoch=24, lrate=0.100, error=1.656\n",
            ">epoch=25, lrate=0.100, error=1.643\n",
            ">epoch=26, lrate=0.100, error=1.631\n",
            ">epoch=27, lrate=0.100, error=1.618\n",
            ">epoch=28, lrate=0.100, error=1.605\n",
            ">epoch=29, lrate=0.100, error=1.593\n",
            ">epoch=30, lrate=0.100, error=1.581\n",
            ">epoch=31, lrate=0.100, error=1.569\n",
            ">epoch=32, lrate=0.100, error=1.557\n",
            ">epoch=33, lrate=0.100, error=1.545\n",
            ">epoch=34, lrate=0.100, error=1.533\n",
            ">epoch=35, lrate=0.100, error=1.522\n",
            ">epoch=36, lrate=0.100, error=1.510\n",
            ">epoch=37, lrate=0.100, error=1.499\n",
            ">epoch=38, lrate=0.100, error=1.487\n",
            ">epoch=39, lrate=0.100, error=1.476\n",
            ">epoch=40, lrate=0.100, error=1.465\n",
            ">epoch=41, lrate=0.100, error=1.454\n",
            ">epoch=42, lrate=0.100, error=1.444\n",
            ">epoch=43, lrate=0.100, error=1.433\n",
            ">epoch=44, lrate=0.100, error=1.422\n",
            ">epoch=45, lrate=0.100, error=1.412\n",
            ">epoch=46, lrate=0.100, error=1.401\n",
            ">epoch=47, lrate=0.100, error=1.391\n",
            ">epoch=48, lrate=0.100, error=1.381\n",
            ">epoch=49, lrate=0.100, error=1.371\n",
            "Scores: [100.0, 100.0, 50.0, 0.0, 100.0]\n",
            "Mean Accuracy: 70.000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement linear regression algorithm from scratch (preferably in python) and test your code on a tiny dataset\n",
        "of your choice. You might want to break down the full algorithm into small functions."
      ],
      "metadata": {
        "id": "hFVlZ5aLHbaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "# Make a prediction with coefficients\n",
        "def predict(row, w):\n",
        "  yhat = w[-1] # bias is at dimnesion 0\n",
        "  for i in range(len(row)-1):\n",
        "    yhat += w[i] * row[i]\n",
        "  return yhat"
      ],
      "metadata": {
        "id": "o87mo6MkMl8k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test step 1\n",
        "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
        "weights = [0.84, 0.24] # these are final weights\n",
        "for row in dataset:\n",
        " yhat = predict(row, weights)\n",
        " print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYX9_ApqNOIo",
        "outputId": "7fbb5abc-1a2c-421b-9b32-b07838f33ed4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=1.000, Predicted=1.080\n",
            "Expected=3.000, Predicted=1.920\n",
            "Expected=3.000, Predicted=3.600\n",
            "Expected=2.000, Predicted=2.760\n",
            "Expected=5.000, Predicted=4.440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Estimate linear regression coefficients using stochastic gradient descent\n",
        "def weights_sgd(train, l_rate, n_epoch):\n",
        "  w = [0.0 for i in range(len(train[0]))]\n",
        "  for epoch in range(n_epoch):\n",
        "    sum_error = 0\n",
        "    for row in train:\n",
        "      yhat = predict(row, w)\n",
        "      error = row[-1]- yhat\n",
        "      sum_error += error**2\n",
        "      w[-1] = w[-1] + l_rate * error\n",
        "      for i in range(len(row)-1):\n",
        "        w[i] = w[i] + l_rate * error * row[i]\n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f'% (epoch, l_rate, sum_error))\n",
        "  return w"
      ],
      "metadata": {
        "id": "x92Y0nx1NTgY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_rate = 0.001\n",
        "n_epoch = 50\n",
        "w = weights_sgd(dataset, l_rate, n_epoch)\n",
        "print(w)\n",
        "for row in dataset:\n",
        "  yhat = predict(row, w)\n",
        "  print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1OqH-7_NhcA",
        "outputId": "624f0e62-6fb4-4f42-d394-8675979b932b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=0, lrate=0.001, error=46.236\n",
            ">epoch=1, lrate=0.001, error=41.305\n",
            ">epoch=2, lrate=0.001, error=36.930\n",
            ">epoch=3, lrate=0.001, error=33.047\n",
            ">epoch=4, lrate=0.001, error=29.601\n",
            ">epoch=5, lrate=0.001, error=26.543\n",
            ">epoch=6, lrate=0.001, error=23.830\n",
            ">epoch=7, lrate=0.001, error=21.422\n",
            ">epoch=8, lrate=0.001, error=19.285\n",
            ">epoch=9, lrate=0.001, error=17.389\n",
            ">epoch=10, lrate=0.001, error=15.706\n",
            ">epoch=11, lrate=0.001, error=14.213\n",
            ">epoch=12, lrate=0.001, error=12.888\n",
            ">epoch=13, lrate=0.001, error=11.712\n",
            ">epoch=14, lrate=0.001, error=10.668\n",
            ">epoch=15, lrate=0.001, error=9.742\n",
            ">epoch=16, lrate=0.001, error=8.921\n",
            ">epoch=17, lrate=0.001, error=8.191\n",
            ">epoch=18, lrate=0.001, error=7.544\n",
            ">epoch=19, lrate=0.001, error=6.970\n",
            ">epoch=20, lrate=0.001, error=6.461\n",
            ">epoch=21, lrate=0.001, error=6.009\n",
            ">epoch=22, lrate=0.001, error=5.607\n",
            ">epoch=23, lrate=0.001, error=5.251\n",
            ">epoch=24, lrate=0.001, error=4.935\n",
            ">epoch=25, lrate=0.001, error=4.655\n",
            ">epoch=26, lrate=0.001, error=4.406\n",
            ">epoch=27, lrate=0.001, error=4.186\n",
            ">epoch=28, lrate=0.001, error=3.990\n",
            ">epoch=29, lrate=0.001, error=3.816\n",
            ">epoch=30, lrate=0.001, error=3.662\n",
            ">epoch=31, lrate=0.001, error=3.525\n",
            ">epoch=32, lrate=0.001, error=3.404\n",
            ">epoch=33, lrate=0.001, error=3.296\n",
            ">epoch=34, lrate=0.001, error=3.200\n",
            ">epoch=35, lrate=0.001, error=3.115\n",
            ">epoch=36, lrate=0.001, error=3.040\n",
            ">epoch=37, lrate=0.001, error=2.973\n",
            ">epoch=38, lrate=0.001, error=2.914\n",
            ">epoch=39, lrate=0.001, error=2.862\n",
            ">epoch=40, lrate=0.001, error=2.815\n",
            ">epoch=41, lrate=0.001, error=2.773\n",
            ">epoch=42, lrate=0.001, error=2.737\n",
            ">epoch=43, lrate=0.001, error=2.704\n",
            ">epoch=44, lrate=0.001, error=2.675\n",
            ">epoch=45, lrate=0.001, error=2.650\n",
            ">epoch=46, lrate=0.001, error=2.627\n",
            ">epoch=47, lrate=0.001, error=2.607\n",
            ">epoch=48, lrate=0.001, error=2.589\n",
            ">epoch=49, lrate=0.001, error=2.573\n",
            "[0.8017220304137576, 0.22998234937311363]\n",
            "Expected=1.000, Predicted=1.032 [1]\n",
            "Expected=3.000, Predicted=1.833 [2]\n",
            "Expected=3.000, Predicted=3.437 [3]\n",
            "Expected=2.000, Predicted=2.635 [3]\n",
            "Expected=5.000, Predicted=4.239 [4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "from math import sqrt\n",
        "# linear Regression Algorithm With Stochastic Gradient Descent\n",
        "def linear_regression(train, test, l_rate, n_epoch):\n",
        "  predictions = list()\n",
        "  w = weights_sgd(train, l_rate, n_epoch)\n",
        "  for row in test:\n",
        "    yhat = predict(row, w)\n",
        "    predictions.append(yhat)\n",
        "  return(predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sn34_EcFN6NK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed(1)\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.001\n",
        "n_epoch = 50\n",
        "scores = evaluate_algorithm(dataset, linear_regression,\n",
        " n_folds, l_rate, n_epoch)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean RMSE: %.3f'\n",
        " % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe-Hw5CvOi0F",
        "outputId": "2db57667-0206-4a9d-ee7a-579181bb427f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=0, lrate=0.001, error=1.310\n",
            ">epoch=1, lrate=0.001, error=1.299\n",
            ">epoch=2, lrate=0.001, error=1.289\n",
            ">epoch=3, lrate=0.001, error=1.278\n",
            ">epoch=4, lrate=0.001, error=1.268\n",
            ">epoch=5, lrate=0.001, error=1.258\n",
            ">epoch=6, lrate=0.001, error=1.248\n",
            ">epoch=7, lrate=0.001, error=1.238\n",
            ">epoch=8, lrate=0.001, error=1.228\n",
            ">epoch=9, lrate=0.001, error=1.219\n",
            ">epoch=10, lrate=0.001, error=1.209\n",
            ">epoch=11, lrate=0.001, error=1.200\n",
            ">epoch=12, lrate=0.001, error=1.190\n",
            ">epoch=13, lrate=0.001, error=1.181\n",
            ">epoch=14, lrate=0.001, error=1.172\n",
            ">epoch=15, lrate=0.001, error=1.163\n",
            ">epoch=16, lrate=0.001, error=1.154\n",
            ">epoch=17, lrate=0.001, error=1.145\n",
            ">epoch=18, lrate=0.001, error=1.136\n",
            ">epoch=19, lrate=0.001, error=1.127\n",
            ">epoch=20, lrate=0.001, error=1.119\n",
            ">epoch=21, lrate=0.001, error=1.110\n",
            ">epoch=22, lrate=0.001, error=1.102\n",
            ">epoch=23, lrate=0.001, error=1.093\n",
            ">epoch=24, lrate=0.001, error=1.085\n",
            ">epoch=25, lrate=0.001, error=1.077\n",
            ">epoch=26, lrate=0.001, error=1.069\n",
            ">epoch=27, lrate=0.001, error=1.061\n",
            ">epoch=28, lrate=0.001, error=1.053\n",
            ">epoch=29, lrate=0.001, error=1.045\n",
            ">epoch=30, lrate=0.001, error=1.037\n",
            ">epoch=31, lrate=0.001, error=1.029\n",
            ">epoch=32, lrate=0.001, error=1.022\n",
            ">epoch=33, lrate=0.001, error=1.014\n",
            ">epoch=34, lrate=0.001, error=1.007\n",
            ">epoch=35, lrate=0.001, error=0.999\n",
            ">epoch=36, lrate=0.001, error=0.992\n",
            ">epoch=37, lrate=0.001, error=0.985\n",
            ">epoch=38, lrate=0.001, error=0.978\n",
            ">epoch=39, lrate=0.001, error=0.971\n",
            ">epoch=40, lrate=0.001, error=0.964\n",
            ">epoch=41, lrate=0.001, error=0.957\n",
            ">epoch=42, lrate=0.001, error=0.950\n",
            ">epoch=43, lrate=0.001, error=0.943\n",
            ">epoch=44, lrate=0.001, error=0.937\n",
            ">epoch=45, lrate=0.001, error=0.930\n",
            ">epoch=46, lrate=0.001, error=0.923\n",
            ">epoch=47, lrate=0.001, error=0.917\n",
            ">epoch=48, lrate=0.001, error=0.910\n",
            ">epoch=49, lrate=0.001, error=0.904\n",
            ">epoch=0, lrate=0.001, error=1.558\n",
            ">epoch=1, lrate=0.001, error=1.542\n",
            ">epoch=2, lrate=0.001, error=1.527\n",
            ">epoch=3, lrate=0.001, error=1.512\n",
            ">epoch=4, lrate=0.001, error=1.497\n",
            ">epoch=5, lrate=0.001, error=1.483\n",
            ">epoch=6, lrate=0.001, error=1.468\n",
            ">epoch=7, lrate=0.001, error=1.454\n",
            ">epoch=8, lrate=0.001, error=1.440\n",
            ">epoch=9, lrate=0.001, error=1.426\n",
            ">epoch=10, lrate=0.001, error=1.412\n",
            ">epoch=11, lrate=0.001, error=1.399\n",
            ">epoch=12, lrate=0.001, error=1.385\n",
            ">epoch=13, lrate=0.001, error=1.372\n",
            ">epoch=14, lrate=0.001, error=1.359\n",
            ">epoch=15, lrate=0.001, error=1.346\n",
            ">epoch=16, lrate=0.001, error=1.333\n",
            ">epoch=17, lrate=0.001, error=1.320\n",
            ">epoch=18, lrate=0.001, error=1.307\n",
            ">epoch=19, lrate=0.001, error=1.295\n",
            ">epoch=20, lrate=0.001, error=1.283\n",
            ">epoch=21, lrate=0.001, error=1.270\n",
            ">epoch=22, lrate=0.001, error=1.258\n",
            ">epoch=23, lrate=0.001, error=1.246\n",
            ">epoch=24, lrate=0.001, error=1.235\n",
            ">epoch=25, lrate=0.001, error=1.223\n",
            ">epoch=26, lrate=0.001, error=1.211\n",
            ">epoch=27, lrate=0.001, error=1.200\n",
            ">epoch=28, lrate=0.001, error=1.189\n",
            ">epoch=29, lrate=0.001, error=1.178\n",
            ">epoch=30, lrate=0.001, error=1.167\n",
            ">epoch=31, lrate=0.001, error=1.156\n",
            ">epoch=32, lrate=0.001, error=1.145\n",
            ">epoch=33, lrate=0.001, error=1.134\n",
            ">epoch=34, lrate=0.001, error=1.124\n",
            ">epoch=35, lrate=0.001, error=1.113\n",
            ">epoch=36, lrate=0.001, error=1.103\n",
            ">epoch=37, lrate=0.001, error=1.093\n",
            ">epoch=38, lrate=0.001, error=1.083\n",
            ">epoch=39, lrate=0.001, error=1.073\n",
            ">epoch=40, lrate=0.001, error=1.063\n",
            ">epoch=41, lrate=0.001, error=1.053\n",
            ">epoch=42, lrate=0.001, error=1.044\n",
            ">epoch=43, lrate=0.001, error=1.034\n",
            ">epoch=44, lrate=0.001, error=1.025\n",
            ">epoch=45, lrate=0.001, error=1.016\n",
            ">epoch=46, lrate=0.001, error=1.006\n",
            ">epoch=47, lrate=0.001, error=0.997\n",
            ">epoch=48, lrate=0.001, error=0.988\n",
            ">epoch=49, lrate=0.001, error=0.979\n",
            ">epoch=0, lrate=0.001, error=1.496\n",
            ">epoch=1, lrate=0.001, error=1.484\n",
            ">epoch=2, lrate=0.001, error=1.472\n",
            ">epoch=3, lrate=0.001, error=1.460\n",
            ">epoch=4, lrate=0.001, error=1.448\n",
            ">epoch=5, lrate=0.001, error=1.436\n",
            ">epoch=6, lrate=0.001, error=1.424\n",
            ">epoch=7, lrate=0.001, error=1.412\n",
            ">epoch=8, lrate=0.001, error=1.401\n",
            ">epoch=9, lrate=0.001, error=1.389\n",
            ">epoch=10, lrate=0.001, error=1.378\n",
            ">epoch=11, lrate=0.001, error=1.367\n",
            ">epoch=12, lrate=0.001, error=1.355\n",
            ">epoch=13, lrate=0.001, error=1.344\n",
            ">epoch=14, lrate=0.001, error=1.334\n",
            ">epoch=15, lrate=0.001, error=1.323\n",
            ">epoch=16, lrate=0.001, error=1.312\n",
            ">epoch=17, lrate=0.001, error=1.302\n",
            ">epoch=18, lrate=0.001, error=1.291\n",
            ">epoch=19, lrate=0.001, error=1.281\n",
            ">epoch=20, lrate=0.001, error=1.271\n",
            ">epoch=21, lrate=0.001, error=1.260\n",
            ">epoch=22, lrate=0.001, error=1.250\n",
            ">epoch=23, lrate=0.001, error=1.240\n",
            ">epoch=24, lrate=0.001, error=1.231\n",
            ">epoch=25, lrate=0.001, error=1.221\n",
            ">epoch=26, lrate=0.001, error=1.211\n",
            ">epoch=27, lrate=0.001, error=1.202\n",
            ">epoch=28, lrate=0.001, error=1.192\n",
            ">epoch=29, lrate=0.001, error=1.183\n",
            ">epoch=30, lrate=0.001, error=1.174\n",
            ">epoch=31, lrate=0.001, error=1.165\n",
            ">epoch=32, lrate=0.001, error=1.155\n",
            ">epoch=33, lrate=0.001, error=1.146\n",
            ">epoch=34, lrate=0.001, error=1.138\n",
            ">epoch=35, lrate=0.001, error=1.129\n",
            ">epoch=36, lrate=0.001, error=1.120\n",
            ">epoch=37, lrate=0.001, error=1.111\n",
            ">epoch=38, lrate=0.001, error=1.103\n",
            ">epoch=39, lrate=0.001, error=1.094\n",
            ">epoch=40, lrate=0.001, error=1.086\n",
            ">epoch=41, lrate=0.001, error=1.078\n",
            ">epoch=42, lrate=0.001, error=1.070\n",
            ">epoch=43, lrate=0.001, error=1.062\n",
            ">epoch=44, lrate=0.001, error=1.053\n",
            ">epoch=45, lrate=0.001, error=1.046\n",
            ">epoch=46, lrate=0.001, error=1.038\n",
            ">epoch=47, lrate=0.001, error=1.030\n",
            ">epoch=48, lrate=0.001, error=1.022\n",
            ">epoch=49, lrate=0.001, error=1.014\n",
            ">epoch=0, lrate=0.001, error=1.310\n",
            ">epoch=1, lrate=0.001, error=1.301\n",
            ">epoch=2, lrate=0.001, error=1.292\n",
            ">epoch=3, lrate=0.001, error=1.283\n",
            ">epoch=4, lrate=0.001, error=1.274\n",
            ">epoch=5, lrate=0.001, error=1.265\n",
            ">epoch=6, lrate=0.001, error=1.256\n",
            ">epoch=7, lrate=0.001, error=1.248\n",
            ">epoch=8, lrate=0.001, error=1.239\n",
            ">epoch=9, lrate=0.001, error=1.231\n",
            ">epoch=10, lrate=0.001, error=1.222\n",
            ">epoch=11, lrate=0.001, error=1.214\n",
            ">epoch=12, lrate=0.001, error=1.206\n",
            ">epoch=13, lrate=0.001, error=1.197\n",
            ">epoch=14, lrate=0.001, error=1.189\n",
            ">epoch=15, lrate=0.001, error=1.181\n",
            ">epoch=16, lrate=0.001, error=1.173\n",
            ">epoch=17, lrate=0.001, error=1.165\n",
            ">epoch=18, lrate=0.001, error=1.157\n",
            ">epoch=19, lrate=0.001, error=1.150\n",
            ">epoch=20, lrate=0.001, error=1.142\n",
            ">epoch=21, lrate=0.001, error=1.134\n",
            ">epoch=22, lrate=0.001, error=1.127\n",
            ">epoch=23, lrate=0.001, error=1.119\n",
            ">epoch=24, lrate=0.001, error=1.112\n",
            ">epoch=25, lrate=0.001, error=1.105\n",
            ">epoch=26, lrate=0.001, error=1.097\n",
            ">epoch=27, lrate=0.001, error=1.090\n",
            ">epoch=28, lrate=0.001, error=1.083\n",
            ">epoch=29, lrate=0.001, error=1.076\n",
            ">epoch=30, lrate=0.001, error=1.069\n",
            ">epoch=31, lrate=0.001, error=1.062\n",
            ">epoch=32, lrate=0.001, error=1.055\n",
            ">epoch=33, lrate=0.001, error=1.049\n",
            ">epoch=34, lrate=0.001, error=1.042\n",
            ">epoch=35, lrate=0.001, error=1.035\n",
            ">epoch=36, lrate=0.001, error=1.029\n",
            ">epoch=37, lrate=0.001, error=1.022\n",
            ">epoch=38, lrate=0.001, error=1.016\n",
            ">epoch=39, lrate=0.001, error=1.009\n",
            ">epoch=40, lrate=0.001, error=1.003\n",
            ">epoch=41, lrate=0.001, error=0.997\n",
            ">epoch=42, lrate=0.001, error=0.990\n",
            ">epoch=43, lrate=0.001, error=0.984\n",
            ">epoch=44, lrate=0.001, error=0.978\n",
            ">epoch=45, lrate=0.001, error=0.972\n",
            ">epoch=46, lrate=0.001, error=0.966\n",
            ">epoch=47, lrate=0.001, error=0.960\n",
            ">epoch=48, lrate=0.001, error=0.954\n",
            ">epoch=49, lrate=0.001, error=0.948\n",
            ">epoch=0, lrate=0.001, error=0.561\n",
            ">epoch=1, lrate=0.001, error=0.557\n",
            ">epoch=2, lrate=0.001, error=0.554\n",
            ">epoch=3, lrate=0.001, error=0.550\n",
            ">epoch=4, lrate=0.001, error=0.546\n",
            ">epoch=5, lrate=0.001, error=0.542\n",
            ">epoch=6, lrate=0.001, error=0.539\n",
            ">epoch=7, lrate=0.001, error=0.535\n",
            ">epoch=8, lrate=0.001, error=0.531\n",
            ">epoch=9, lrate=0.001, error=0.528\n",
            ">epoch=10, lrate=0.001, error=0.524\n",
            ">epoch=11, lrate=0.001, error=0.521\n",
            ">epoch=12, lrate=0.001, error=0.517\n",
            ">epoch=13, lrate=0.001, error=0.514\n",
            ">epoch=14, lrate=0.001, error=0.510\n",
            ">epoch=15, lrate=0.001, error=0.507\n",
            ">epoch=16, lrate=0.001, error=0.503\n",
            ">epoch=17, lrate=0.001, error=0.500\n",
            ">epoch=18, lrate=0.001, error=0.497\n",
            ">epoch=19, lrate=0.001, error=0.493\n",
            ">epoch=20, lrate=0.001, error=0.490\n",
            ">epoch=21, lrate=0.001, error=0.487\n",
            ">epoch=22, lrate=0.001, error=0.484\n",
            ">epoch=23, lrate=0.001, error=0.480\n",
            ">epoch=24, lrate=0.001, error=0.477\n",
            ">epoch=25, lrate=0.001, error=0.474\n",
            ">epoch=26, lrate=0.001, error=0.471\n",
            ">epoch=27, lrate=0.001, error=0.468\n",
            ">epoch=28, lrate=0.001, error=0.465\n",
            ">epoch=29, lrate=0.001, error=0.462\n",
            ">epoch=30, lrate=0.001, error=0.459\n",
            ">epoch=31, lrate=0.001, error=0.456\n",
            ">epoch=32, lrate=0.001, error=0.453\n",
            ">epoch=33, lrate=0.001, error=0.450\n",
            ">epoch=34, lrate=0.001, error=0.447\n",
            ">epoch=35, lrate=0.001, error=0.445\n",
            ">epoch=36, lrate=0.001, error=0.442\n",
            ">epoch=37, lrate=0.001, error=0.439\n",
            ">epoch=38, lrate=0.001, error=0.436\n",
            ">epoch=39, lrate=0.001, error=0.433\n",
            ">epoch=40, lrate=0.001, error=0.431\n",
            ">epoch=41, lrate=0.001, error=0.428\n",
            ">epoch=42, lrate=0.001, error=0.425\n",
            ">epoch=43, lrate=0.001, error=0.423\n",
            ">epoch=44, lrate=0.001, error=0.420\n",
            ">epoch=45, lrate=0.001, error=0.418\n",
            ">epoch=46, lrate=0.001, error=0.415\n",
            ">epoch=47, lrate=0.001, error=0.412\n",
            ">epoch=48, lrate=0.001, error=0.410\n",
            ">epoch=49, lrate=0.001, error=0.407\n",
            "Scores: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Mean RMSE: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Please see the house prices dataset (https://www.kaggle.com/competitions/house-prices-advanced-\n",
        "regression-techniques/data) on Kaggle. Use the appropriate algorithm (from question 1 or 2) to learn a model\n",
        "from the training set and predict the prices for the test set.\n",
        "  1. Report your average error in the prediction."
      ],
      "metadata": {
        "id": "Xhq0KdEzHbnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I have decided to use Linear regression to predict the prices for the test set. Logistic regression is used when the goal is to estimate the probability of a binary outcome , while linear regression is used when the goal is to predict a continuous, numerical value, which is similiar to our case.\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "YNKUGOUBSsQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "HOb_1ySASr-A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load test and train data\n",
        "test = pd.read_csv('test.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "rASHE0GdImlW",
        "outputId": "c23ad955-7806-4f3d-b378-eebd4f3695cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f4931334d0d5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load test and train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Null values\n",
        "train.isnull().sum()*100/train.shape[0]"
      ],
      "metadata": {
        "id": "jtWWmSvHJtBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "varlist =  ['CentralAir']\n",
        "\n",
        "# Defining the map function\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, \"no\": 0})\n",
        "\n",
        "# Applying the function to the housing list\n",
        "train[varlist] = train[varlist].apply(binary_map)\n",
        "\n",
        "#Overall Quality is very highliy correlated\n",
        "corr_y = train.corr()\n",
        "corr_y['SalePrice'].sort_values(ascending=False).abs()[1:]\n"
      ],
      "metadata": {
        "id": "MQEYsf8oJ-G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#select continous features only\n",
        "num_columns = [col for col in train.columns if train[col].dtype == 'int64']\n",
        "\n",
        "corr = train[num_columns].corr().abs()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "# plot heatmap\n",
        "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n",
        "            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1yAcPIIBLRZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are now going to get rid of features that have no impact on overall house sold price\n",
        "num_col2 = [x for x in num_columns if x!='Id' and x!='MoSold' ]\n",
        "num_col2 , len(num_col2)\n"
      ],
      "metadata": {
        "id": "G-64R7N-LsF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(train.columns), 5):\n",
        "        sns.pairplot(data=train,\n",
        "                    x_vars=train.columns[i:i+5],\n",
        "                    y_vars=['SalePrice'])"
      ],
      "metadata": {
        "id": "oZbmVm83L_hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have already loaded your data into a DataFrame 'df_train' and defined 'num_col2'\n",
        "\n",
        "# Separate the features (x_train) and target variable (y_train)\n",
        "train = train[num_col2]\n",
        "x_train = train.iloc[:, :-1].values  # Features\n",
        "y_train = np.log1p(train.iloc[:, -1:].values)  # Target variable\n",
        "\n",
        "# Standardize (scale) the features manually\n",
        "mean = x_train.mean(axis=0)\n",
        "std_dev = x_train.std(axis=0)\n",
        "x_train = (x_train - mean) / std_dev\n",
        "\n",
        "# Linear Regression\n",
        "def linear_regression(x_train, y_train):\n",
        "    n = x_train.shape[0]\n",
        "    k = x_train.shape[1]\n",
        "\n",
        "    # Initialize coefficients and intercept\n",
        "    beta = np.zeros((k, 1))\n",
        "    intercept = 0\n",
        "\n",
        "    # Fit the model (Manually implement linear regression)\n",
        "    x_train_with_intercept = np.hstack((np.ones((n, 1)), x_train))\n",
        "    y_pred = np.dot(x_train_with_intercept, np.vstack((intercept, beta)))\n",
        "\n",
        "    # Calculate RMSE (Root Mean Squared Error)\n",
        "    rmse = np.sqrt(np.mean((y_pred - y_train) ** 2))\n",
        "\n",
        "    return rmse\n",
        "\n",
        "# Cross-validation\n",
        "def cross_validation(x_train, y_train, cv=4):\n",
        "    n = x_train.shape[0]\n",
        "    fold_size = n // cv\n",
        "    rmses = []\n",
        "\n",
        "    for i in range(cv):\n",
        "        start = i * fold_size\n",
        "        end = (i + 1) * fold_size\n",
        "\n",
        "        x_val = x_train[start:end, :]\n",
        "        y_val = y_train[start:end, :]\n",
        "\n",
        "        x_train_fold = np.vstack((x_train[:start, :], x_train[end:, :]))\n",
        "        y_train_fold = np.vstack((y_train[:start, :], y_train[end:, :]))\n",
        "\n",
        "        rmse = linear_regression(x_train_fold, y_train_fold)\n",
        "        rmses.append(rmse)\n",
        "\n",
        "    return rmses\n",
        "\n",
        "#cross-validation\n",
        "cv_scores = cross_validation(x_train, y_train, cv=4)\n",
        "print(\"Cross-validation RMSE scores:\", cv_scores)\n",
        "\n",
        "# Calculate Average RMSE\n",
        "average_rmse = np.mean(cv_scores)\n",
        "print(\"Average RMSE:\", average_rmse)\n"
      ],
      "metadata": {
        "id": "lUDdavJuedxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Please modify the above dataset (in question 3) to answer whether the house will sell for 180000 or not and\n",
        "use the appropriate algorithm (from question 1 or 2) to learn a model from the training set and answer\n",
        "whether the prices > 180000 or not for the test set. 30 pts\n",
        "  1. Report your accuracy. 10 pts\n"
      ],
      "metadata": {
        "id": "l8RisSKKHcAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will utilize logistic regression since we are looking for a binary representation of if the house has sold or not (0 or 1). Logistic regression is defintetly the correct choice for this question and will be implemented accordingly below"
      ],
      "metadata": {
        "id": "z495ELzBYVfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the logistic regression function\n",
        "def logistic_regression(train, weights):\n",
        "    predictions = list()\n",
        "    for row in train:\n",
        "        yhat = predict(row, weights)\n",
        "        yhat = round(yhat)\n",
        "        predictions.append(yhat)\n",
        "    return predictions\n",
        "\n",
        "# Define the weights_sgd function\n",
        "def weights_sgd(train, l_rate, n_epoch):\n",
        "    weights = [0.0 for _ in range(len(train[0]))]\n",
        "    for epoch in range(n_epoch):\n",
        "        sum_error = 0\n",
        "        for row in train:\n",
        "            yhat = predict(row, weights)\n",
        "            error = row[-1] - yhat\n",
        "            sum_error += error ** 2\n",
        "            weights[-1] = weights[-1] + l_rate * error\n",
        "            for i in range(len(row) - 1):\n",
        "                weights[i] = weights[i] + l_rate * error * row[i]\n",
        "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "    return weights\n",
        "\n",
        "# Define the updated predict function\n",
        "def predict(row, weights):\n",
        "    activation = weights[0]\n",
        "    for i in range(len(row) - 1):\n",
        "        activation += weights[i + 1] * row[i]\n",
        "    return 1.0 / (1.0 + np.exp(-activation))\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "#1 if SalePrice > 180,000, else 0\n",
        "df['target_class'] = (df['SalePrice'] > 180000).astype(int)\n",
        "\n",
        "# Define features (X) and the binary target variable (y)\n",
        "X = df.drop(columns=['Id', 'SalePrice', 'target_class'])  # Drop unnecessary columns\n",
        "y = df['target_class']  # Binary target variable\n",
        "\n",
        "# Perform one-hot encoding for categorical features\n",
        "X_encoded = pd.get_dummies(X, columns=['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
        "                                       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
        "                                       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
        "                                       'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
        "                                       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n",
        "                                       'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu',\n",
        "                                       'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive',\n",
        "                                       'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition'])\n",
        "\n",
        "# Drop rows with missing values\n",
        "X_encoded.dropna(axis=0, inplace=True)\n",
        "y = y[X_encoded.index]  # Adjust the target variable accordingly\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X_encoded) * split_ratio)\n",
        "\n",
        "X_train = X_encoded.iloc[:split_index]\n",
        "y_train = y.iloc[:split_index]\n",
        "X_test = X_encoded.iloc[split_index:]\n",
        "y_test = y.iloc[split_index:]\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_array = X_train.values\n",
        "X_test_array = X_test.values\n",
        "y_train_array = y_train.values\n",
        "y_test_array = y_test.values\n",
        "\n",
        "# Train a Logistic Regression classifier using custom functions\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "weights = weights_sgd(X_train_array, learning_rate, n_epochs)\n",
        "\n",
        "# Predict whether house prices in the test set are greater than $180,000 or not\n",
        "y_pred = logistic_regression(X_test_array, weights)\n",
        "\n",
        "# Calculate accuracy and generate a classification report\n",
        "accuracy = np.mean(y_pred == y_test_array)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "hci1WMOakfSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}